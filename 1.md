1. Fractal Language Model Stack
Input tokens are mapped through token and positional embeddings and processed by a stack of fractal Transformer blocks.
Each block follows a pre-norm residual structure:
X' = X + FractalAttention(LN1(X))
Y = X' + FractalFFN(LN2(X'))
The fractal formulation introduces recursive or multi-scale internal structure within both attention and feed-forward sublayers, enabling hierarchical feature refinement across logical depth.
The final normalized representation is projected through an LM head to produce output logits.
2. Fractal Transformer Block
Unlike a standard Transformer block, the attention and feed-forward modules are internally structured to reflect fractal decomposition.
This preserves residual continuity while enabling:
• Multi-scale representation mixing
• Self-similar parameter reuse
• Implicit hierarchical feature extraction
The block maintains standard residual-add semantics but modifies the internal transformation topology.
3. Multi-Fractal Linear Core
The output projection is not a conventional dense matrix.
Instead, the final weight matrix is constructed through a fractal generation process:
Learnable seed matrices (4×4)
Low-bit straight-through quantization (BitSTE, 1.58-bit)
Kronecker expansion for recursive dimensional growth
Cropping and scaling to target dimensionality
Summation to form the final weight matrix
Formally:
W = ∑iSi⊗Ki
where ⊗ denotes the Kronecker product expansion.
The final linear transformation is:
y = x W
This approach enforces structured parameterization and reduces storage complexity by generating large matrices from compact fractal seeds.